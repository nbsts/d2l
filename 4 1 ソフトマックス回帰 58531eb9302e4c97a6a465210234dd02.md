# 4.1. ソフトマックス回帰

日付: 2023年6月26日

- 回帰は「**どれくらい？**（How much?/How many?）」の問題について考えるためのツールである。
- 教師あり学習は回帰だけではない。この節では、「**どの種類か？**」に着目する**分類問題**を取り扱う。
    - このメールは、迷惑メールフォルダと受信箱のどちらに入れるべきか？
    - この顧客はサブスクリプションサービスに登録するか、しないか？
    - この画像は、ロバ、犬、猫、雄鶏のどれを表すか？
    - アストン君が次に見そうな映画はどれか？
    - ユーザが次に読むのは本の中のどのセクションか？
- 機械学習屋さんは、次の2種類の問題を「分類問題」と一括りにしてしまう。
    1. 「**ハード**」な分類：事例がどのカテゴリに属すかを知る問題
    2.  「**ソフト**」な分類：事例がそれぞれのカテゴリに属する可能性を知る問題
    
    これら2つの差異は微妙であり、また曖昧である。例えば、ハードな分類問題の答えを知るためにソフトな分類問題のためのモデルを使うことがある。
    
- 正解のラベルが1つとは限らない。例えば、1つのニュース記事がエンタメ、ビジネス、宇宙旅行の話題を同時に含む場合もある。この問題は、多ラベル分類（Multi-label classification）として知られている。

# 4.1.1. クラス分類

- 簡単な画像分類問題から始める。ここでは、入力は$2\times 2$のモノクロ画像とする。4つのピクセルそれぞれの値はスカラ値$x_1, x_2, x_3, x_4$で表すことができる。画像は「猫」「ニワトリ」「犬」のいずれか1つに属するとする。
- ラベルをどのように表現するかを考える。$y \in \{1, 2, 3\}$として各整数に犬、猫、ニワトリを割り振りたいと思うかも知れない。この手法は、ラベルに元々順番がある場合（例えば、乳児、幼児、少年、青年、大人、老人の分類）には適するが、今回はそうはいかない。
- カテゴリカルデータを表現する簡単な方法はone-hotエンコーディングである。カテゴリ数と同じだけの次元を持つベクトルを用意し、特定の要素のみを1、ほかを0にする。今回は、猫に$(1,0,0)$、ニワトリに$(0,1,0)$、犬に$(0,0,1)$のベクトルを割り振る。
    
    $$
    y \in \{(1, 0, 0), (0, 1, 0), (0, 0, 1)\}
    $$
    

## 4.1.1.1. 線形モデル

- クラスの確率を予測するには、それぞれのクラスに対応した複数の出力を持つモデルが必要となる。線形モデルにおいては、出力の数だけ線形関数が必要である。ここでは、 入力となる特徴量4つに対して出力が3つなので、重みを意味する$(4×3 =) 12$個のスカラ値（$w_{ij}$）とバイアスを意味する3つのスカラ値（$b_{i}$）が必要になる。
    
    $$
    o_1 = x_1 w_{11} + x_2 w_{12} + x_3 w_{13} + x_4 w_{14} + b_1\\o_2= x_1 w_{21} + x_2 w_{22} + x_3 w_{23} + x_4 w_{24} + b_2\\o_3= x_1 w_{31} + x_2 w_{32} + x_3 w_{33} + x_4 w_{34} + b_3
    $$
    
- 線形回帰のときと同様、単層ニューラルネットワークを用いる。出力はどれもすべての入力に依存するから、出力層は「**全結合層**」とも呼ばれる。

![https://d2l.ai/_images/softmaxreg.svg](https://d2l.ai/_images/softmaxreg.svg)

- より簡潔な記述のために、ベクトルと行列を用いて表現する。重みを$3\times4$行列$W$に、バイアスを3次元ベクトル$\bm{b}$にまとめて、$\bm{o}=W\bm{x}+\bm{b}$と表す事ができる。
    
    $$
    
    \begin{bmatrix}o_1 \\o_2 \\o_3 \\\end{bmatrix}=\begin{bmatrix}w_{11} & w_{12} & w_{13} & w_{14} \\w_{21} & w_{22} & w_{23} & w_{24} \\w_{31} & w_{32} & w_{33} & w_{34} \\\end{bmatrix}\begin{bmatrix}x_1 \\ x_2 \\ x_3 \\ x_4 \\\end{bmatrix}+\begin{bmatrix}b_1 \\b_2 \\b_3 \\\end{bmatrix}
    $$
    

<aside>
💡 たまに、$\bm{o}=\bm{x}W+\bm{b}$のように表記されることがある。これは、列ベクトルを用いているだけのこと。

</aside>

## 4.1.1.2. ソフトマックス

- 分類問題をone-hotベクトルの回帰問題と見ることはうまくいくが、次の欠点がある。
    - 出力$o_i$の和が1になるとは限らない。ソフトな分類問題では入力があるクラスに属する確率を知りたいので、和は1になってほしい。
    - 出力$o_i$が非負であるという保証もない。
- したがって、出力を「圧縮」する必要が生じる。そのためには、指数関数$P(y = i) \propto \exp o_i$が有用である。これは単調増加関数であるから出力が増加するに伴いクラス分類の確率が増加し、また非負の値を返してくれる。
- 和が1になるように、それぞれの値を合計値で割る。これは、**正規化**と呼ばれる。
    
    $$
    \hat{\bm{y}} = \mathrm{softmax}(\bm{o}) \quad \text{where}\quad \hat{y}_i = \frac{\exp(o_i)}{\sum_k \exp(o_k)}
    $$
    
- ソフトマックス関数への入力ベクトルと出力ベクトルを左右に並べると以下のようになる。

![Untitled](4%201%20%E3%82%BD%E3%83%95%E3%83%88%E3%83%9E%E3%83%83%E3%82%AF%E3%82%B9%E5%9B%9E%E5%B8%B0%2058531eb9302e4c97a6a465210234dd02/Untitled.png)

- ソフトマックスは引数の順番を保存するため、どのクラスが最も高い確率になったかを調べるにはソフトマックスを計算する必要はない。

## 4.1.1.3 ベクトル化

- 計算効率向上のため、ミニバッチの計算をベクトル化する。
- $d$次元の入力が$n$個あつまったミニバッチ$X\in \mathbb{R}^{n \times d}$があるとする。さらに、分類する数はqであるとする。すると、重み行列は$W\in \mathbb{R}^{d \times q}$を、バイアスは$\bm{b}\in \mathbb{R}^{1 \times q}$を満たす。
    
    $$
    O=XW+\bm{b},\\ \hat{Y}=\mathrm{softmax}(O)
    $$
    
- こうすることで、主要な計算を行列の積算に落とし込むことができる。ソフトマックスは行ごとに計算される。

# 4.1.2. 損失関数

- ここまでで、特徴量から確率を求める関数を定義することができた。次は、この関数の精度を向上させる方法を考える。ここでは、最尤（maximum likelihood）推定を用いる。

## 4.1.2.1. 対数尤度（Log-Likelihood）

- ソフトマックス関数は、$\hat{y_1}= P(y=\text{猫} \mid \bm{x})$のように、入力のもとでの条件付き確率を表すベクトルを出力する。
- 特徴量のデータセット$X$と、one-hotベクトル形式でのラベルが表現された$Y$が与えられたとする。このとき、データセット全体の**尤度$P(Y \mid X)$**（すべてのラベルを正しく再現できる確率）は、各学習事例の尤度$P(\bm{y}^{(i)} \mid \bm{x}^{(i)})$の独立性を仮定することで次のように表すことができる。
    
    $$
    P(Y \mid X) = \prod_{i=1}^n P(\bm{y}^{(i)} \mid \bm{x}^{(i)})
    $$
    
- 尤度は確率であるので、0から1までの値を取り、1に近いほどモデルが学習データ通りに分類できていることを表す。そこで、より精度良いモデルを作るためには、尤度を大きくするようにパラメータを調整すればよい。このように、尤度を最大化することでパラメータ推定を行うことを**最尤推定**と呼ぶ。
- 最大値を求めるのに、積の形は扱いづらい（コンピュータ上での小さい数の乗算はアンダーフローを引き起こす）ので、代わりに**負の対数尤度**（negative log-likelihood）を最小化する。
    
    $$
    -\log P(Y \mid X) = \sum_{i=1}^n -\log P(\bm{y}^{(i)} \mid \bm{x}^{(i)})= \sum_{i=1}^n l(\bm{y}^{(i)}, \hat{\bm{y}}^{(i)})
    $$
    
- ここで、ある学習事例のラベル$\bm{y}$とその予測値$\hat{\bm{y}}$の組に対して、損失関数は次の通りである。
    
    $$
    l(\bm{y}, \hat{\bm{y}}) = - \sum_{j=1}^q y_j \log \hat{y}_j
    $$
    
- この損失関数は一般に交差エントロピー損失と呼ばれる。なぜこのように呼ばれるかは後述。
- $\bm{y}$はone-hotベクトルであるから、1から$q$までの和は1つの項を残して消える。また、$\hat{\bm{y}}$が確率ベクトルである限りは、$l(\bm{y}, \hat{\bm{y}})$が0を下回ることはない。これが0になるのは、予測ラベルと実際のラベルが完全に一致した場合である。このようなことは、実際には実現されない。なぜなら、ある要素についてソフトマックスの出力$\hat{y}_i$ を1にするには$o_i$ を無限大にするか、$o_i$以外を負の無限大にする必要があるからである。

## 4.1.2.2. ソフトマックスと交差エントロピー損失

- ソフトマックスや交差エントロピー損失の計算についてもう少し詳しく見てみる。$\hat{\bm{y}}$の定義式を交差エントロピーの定義式に代入し、ソフトマックスの定義を用いることで次の式を得る。
    
    $$
    \begin{aligned}l(\bm{y}, \hat{\bm{y}}) &=  - \sum_{j=1}^q y_j \log \frac{\exp(o_j)}{\sum_{k=1}^q \exp(o_k)} \\ &= \sum_{j=1}^q y_j \log \sum_{k=1}^q \exp(o_k) - \sum_{j=1}^q y_j o_j \\&= \log \sum_{k=1}^q \exp(o_k) - \sum_{j=1}^q y_j o_j\end{aligned}
    $$
    
- さらに、これを$o_j$で偏微分したものを考える。
    
    $$
    \frac{\partial (\bm{y}, \hat{\bm{y}})}{\partial o_j} = \frac{\exp(o_j)}{\sum_{k=1}^q \exp(o_k)} - y_j = \mathrm{softmax}(\bm{o})_j - y_j 
    $$
    
- このように、損失の偏導関数は、モデルの出力した確率と実際のデータの差となる。
- $l(\bm{y}, \hat{\bm{y}})$についての式は、$\bm{y}$がone-hotベクトルでなくとも（例えば、$\bm{y}=(0.1, 0.2, 0.7)$である場合でも）用いることができる。この場合、ラベルについての確率分布の損失となる。

# 4.1.3. 情報理論の基礎

- 多くの深層学習に関する多くの書籍や論文は、情報理論の考えや用語を用いている。情報理論は、エンコーディング（符号化）、デコーディング（復号化）、情報の伝達や加工についての問題を取り扱う。

## 4.1.3.1. エントロピー

- 情報理論の中心となる考えは、データに含まれる情報の量を定量化することである。これにより、データの圧縮に限度が設けられる。確率分布$P$に対し、エントロピーは次のように定義される。
    
    $$
    H[P] = \sum_j - P(j) \log P(j)
    $$
    
- 情報についての基礎理論によれば、分布Pから無作為に選ばれたデータをエンコードするには、少なくとも$H[P]$natの情報量が必要になる。natとはbitとほぼ同じ意味であるが、底として2の代わりに$e$を用いたものである。

## 4.1.3.2. サプライザル（驚き）

- データの圧縮と、予測との間にどのような関係があるのだろうか。データの流れを圧縮したいとする。もし次のトークンを簡単に予測することができたら、そのデータは簡単に圧縮できる。極端な例で言えば、値が1つしかないデータの流れは極めて予測が簡単である。いつも値が同じならば、その値を伝えるのに情報は必要ない。データの予測が簡単なら、圧縮も簡単、ということである。
- しかしながら、すべての事象を完璧に予測することは不可能であり、予測できない出来事が起これば我々は「驚き」を覚える。情報理論の祖であるクロード・シャノンは、確率$P(j)$で事象$j$が起こった際の「驚き」を$\log \frac{1}{P(j)} = -\log P(j)$と定式化した。エントロピーは、この「驚き」の期待値として定義されており、情報の不確定度と解釈することができる。

<aside>
💡 事象生起確率$p$の関数である選択情報量$I(p)=-\log p$は次の性質を満たす。

1. **単調性**：事象の生起確率が低いほど、選択情報量が大きい。
    
    $$
    p<q \Leftrightarrow I(p)<I(q)
    $$
    
2. **非負性**：選択情報量は非負であり、これが0になるのは、$P(j)=1$のときである。
    
    $$
    I(p)\geq0
    $$
    
3. **加法性**：●○■□の4つの図形から1つを選んで、その情報を与える場合を考える。
「丸を選んだ」と言われてから「白い図形を選んだ」と言われた時の選択情報量と、「白くて丸い図形を選んだ」と言われた時の選択情報量は同じである。
    
    $$
    I(p)+I(q)=I(pq)
    $$
    
</aside>

## 4.1.3.3. 交差エントロピーの再考

- エントロピーが驚きの度合いを示すなら、交差エントロピーとはなんだろうか？$P$に対する$Q$の交差エントロピー$H(P, Q)$は、確率分布$P$に基づいて生成されたデータを見た場合、主観確率$Q$をもつ観測者の「驚き」の期待値である。
    
    $$
    H(P, Q) \stackrel{\mathrm{def}}{=} \sum_j - P(j) \log Q(j)
    $$
    
- $H(P, Q)$は、$P=Q$の場合に最小値$H(P)$をとる。
- 結局、交差エントロピー分類は
    1. 観測されたデータの尤度の最大化
    2. ラベルの情報を与える際の「驚き」の最小化
    
    の問題として考えることができる。
    

# 参考資料

[機械学習帳](https://chokkan.github.io/mlnote/classification/01binary.html#id7)

情報処理学会 編，岡崎直観・荒瀬由紀・鈴木潤・鶴岡慶雅・宮尾祐介著『自然言語処理の基礎』（2022, オーム社）